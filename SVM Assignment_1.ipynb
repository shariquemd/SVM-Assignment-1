{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d5932f-fbe2-48fb-82c7-b35ef1b4fd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?\n",
    "The mathematical formula for a linear SVM can be expressed as follows:\n",
    "Given a set of input features X and corresponding class labels y, the objective is to find a hyperplane w⋅X+b=0 that separates the data into different classes. Here, w represents the weights associated with each feature, and b is the bias term.\n",
    "\n",
    "Q2. What is the objective function of a linear SVM?\n",
    "The ∑ni=1(ξi)k ∑ i = 1 n ( ξ i ) k is the loss term and C is a HyperParameter which controls the tread-off between maximizing the margin and minimizing the loss. The HyperParameter C is also called as Regularization Constant .\n",
    "\n",
    "\n",
    "Q3. What is the kernel trick in SVM?\n",
    "The kernel trick is a method used in Support Vector Machines (SVMs) to convert data that is not linearly separable into a higher-dimensional feature space where it may be linearly separated. \n",
    "The kernel trick is widely used in SVM models to bridge linearity and non-linearity. It converts non-linear lower dimension space to a higher dimension space so that linear classification can be obtained. \n",
    "The kernel trick is a simple method where non-linear data is projected onto a higher dimension space so that it can be easier to classify the data where it could be linearly divided by a plane. \n",
    "The kernel trick is to convert dot product of support vectors to the dot product of mapping function. \n",
    "The kernel trick can risk overfitting if the dimensions are chosen to be really large. \n",
    "\n",
    "\n",
    "\n",
    "Q4. What is the role of support vectors in SVM? Explain with an example.\n",
    "\n",
    "Support vectors are the data points that lie closest to the decision boundary (margin) between different classes. They play a crucial role in defining the optimal hyperplane because they are the ones that contribute to the margin and influence the position of the decision boundary.\n",
    "\n",
    "Example:\n",
    "Consider a binary classification problem with two classes, positive (+1) and negative (-1). The support vectors are the data points from both classes that are closest to the decision boundary. These points are crucial because they determine the margin, and any change in their position could potentially alter the decision boundary.\n",
    "\n",
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin, and Hard margin in SVM?\n",
    "\n",
    "Let's create visualizations for these concepts:\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Select only two features for visualization\n",
    "y = (iris.target != 0) * 1  # Binary classification: Setosa vs. Others\n",
    "\n",
    "# Fit SVM models with different margins\n",
    "models = {'Hard Margin': SVC(kernel='linear', C=1e10),\n",
    "          'Soft Margin': SVC(kernel='linear', C=0.1)}\n",
    "\n",
    "# Plot the hyperplane, marginal planes, and support vectors\n",
    "plt.figure(figsize=(12, 5))\n",
    "for i, (name, model) in enumerate(models.items(), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Plot data points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='k', label='Data points')\n",
    "    \n",
    "    # Plot support vectors\n",
    "    plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
    "                facecolors='none', edgecolors='k', s=100, label='Support Vectors')\n",
    "    \n",
    "    # Plot decision boundary and margins\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50),\n",
    "                         np.linspace(ylim[0], ylim[1], 50))\n",
    "    \n",
    "    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot hyperplane\n",
    "    plt.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "                linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # Plot margins\n",
    "    plt.scatter([], [], c='k', marker='-', label='Margins')\n",
    "    \n",
    "    plt.title(name)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "This code creates visualizations for a hard-margin SVM and a soft-margin SVM, illustrating the hyperplane, marginal planes, and support vectors for each case.\n",
    "\n",
    "Q6. SVM Implementation through Iris dataset.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train a linear SVM classifier\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the SVM classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
